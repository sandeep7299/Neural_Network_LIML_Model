{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# nn_liml_realdata.py  (Real-data NN first stage + LIML L1)\n",
        "# - Endogenous: LABOR, COWS, FEED, ROUGHAGE\n",
        "# - Exogenous in X: CONST, LAND, YEAR dummies (as in the paper)\n",
        "# - Z: CONST, LAND, PMILK, PFEED, LANDOWN, COOP dummies, YEAR dummies\n",
        "# - Augments Z up to \"C\": squares + selected interactions\n",
        "# - First-stage baselines: OLS and ElasticNetCV\n",
        "# - PCA for NN first-stage (keep 95% variance), optional for OLS/EN\n",
        "# - Adds: standardized Z cond number, OPG SEs, TE summary, Σ̂ηη spectrum\n",
        "# - Variance bounds/penalties relaxed to avoid boundary sticking\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional, Dict, Any, List\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from scipy.linalg import cho_factor, cho_solve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# ---------------- Global setup ----------------\n",
        "BASE_SEED = 12345\n",
        "np.random.seed(BASE_SEED)\n",
        "torch.manual_seed(BASE_SEED)\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "# ---------------- Config toggles ----------------\n",
        "PCA_Z_FOR_NN = False           # PCA compress Z before NN first-stage\n",
        "PCA_Z_FOR_OLS_LASSO = False    # Use same PCA-compressed Z in OLS/ElasticNet baselines\n",
        "PCA_VAR_KEEP = 0.95           # fraction of variance to retain in PCA for NN/baselines\n",
        "\n",
        "# ==============================\n",
        "# Utilities\n",
        "# ==============================\n",
        "def make_spd_from_symmetric(A: np.ndarray, jitter: float = 1e-8) -> np.ndarray:\n",
        "    \"\"\"Nearest-ish SPD: symmetrize + add jitter if needed (checked by Cholesky).\"\"\"\n",
        "    A = 0.5*(A + A.T)\n",
        "    try:\n",
        "        cho_factor(A, lower=False, check_finite=False)\n",
        "        return A\n",
        "    except Exception:\n",
        "        bump = max(jitter, 1e-6*(1.0 + float(np.max(np.abs(np.diag(A))) if A.size else 0.0)))\n",
        "        while True:\n",
        "            A_try = A + bump*np.eye(A.shape[0])\n",
        "            try:\n",
        "                cho_factor(A_try, lower=False, check_finite=False)\n",
        "                return A_try\n",
        "            except Exception:\n",
        "                bump *= 10.0\n",
        "\n",
        "def standardize_fit(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True)\n",
        "    sd = np.where(sd < 1e-8, 1.0, sd)\n",
        "    return (X - mu)/sd, mu, sd\n",
        "\n",
        "def standardize_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray) -> np.ndarray:\n",
        "    return (X - mu)/sd\n",
        "\n",
        "def kfold_indices(n: int, K: int, seed: int = 12345):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    folds = np.array_split(idx, K)\n",
        "    for k in range(K):\n",
        "        te = folds[k]\n",
        "        tr = np.hstack([folds[j] for j in range(K) if j != k])\n",
        "        yield tr, te\n",
        "\n",
        "def r2_score(y, yhat):\n",
        "    sst = np.sum((y - y.mean())**2)\n",
        "    sse = np.sum((y - yhat)**2)\n",
        "    return 1.0 - sse / max(sst, 1e-12)\n",
        "\n",
        "def te_from_r_sigma(r: np.ndarray, sigma_u: float, sigma_c: float) -> np.ndarray:\n",
        "    \"\"\"Jondrow et al. E[exp(-u)|ε=r] for half-normal u and Normal v.\"\"\"\n",
        "    sigma2 = sigma_u**2 + sigma_c**2\n",
        "    mu_star = -r * (sigma_u**2) / sigma2\n",
        "    sigma_star = (sigma_u * sigma_c) / np.sqrt(sigma2)\n",
        "    a = mu_star / (sigma_star + 1e-15)\n",
        "    log_ratio = norm.logcdf(a - sigma_star) - norm.logcdf(a)\n",
        "    return np.exp(-mu_star + 0.5 * sigma_star**2 + log_ratio)\n",
        "\n",
        "def _round_list(xs, nd=4):\n",
        "    # Robust to numpy arrays / scalars\n",
        "    try:\n",
        "        return [round(float(v), nd) for v in xs]\n",
        "    except TypeError:\n",
        "        return [round(float(xs), nd)]\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# NN first stage (small-n stable)\n",
        "# ==============================\n",
        "@dataclass\n",
        "class NNConfig:\n",
        "    epochs: int = 800\n",
        "    batch_size: int = 32\n",
        "    lr: float = 2e-4\n",
        "    weight_decay: float = 1e-3\n",
        "    hidden: Tuple[int, ...] = (128, 64)\n",
        "    dropout: float = 0.45\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = BASE_SEED\n",
        "    K: int = 10\n",
        "\n",
        "class RF_NN(nn.Module):\n",
        "    \"\"\"Compact MLP + learned residual covariance via Cholesky parameter Lpar.\"\"\"\n",
        "    def __init__(self, z_dim: int, k2: int, hidden=(128, 64), dropout=0.45):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = z_dim\n",
        "        for h in hidden:\n",
        "            layers += [\n",
        "                nn.Linear(prev, h),\n",
        "                nn.ELU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, k2)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        self.Lpar = nn.Parameter(torch.eye(k2) * 0.5)\n",
        "\n",
        "    def _Sigma(self):\n",
        "        L = torch.tril(self.Lpar)\n",
        "        d = F.softplus(torch.diag(L)) + 0.5  # keep positive diagonal\n",
        "        L = L - torch.diag(torch.diag(L)) + torch.diag(d)\n",
        "        return L @ L.T, L\n",
        "\n",
        "    def nll(self, Zb, X2b):\n",
        "        \"\"\"Gaussian MLE on residuals with full Σ_res = LLᵀ.\"\"\"\n",
        "        X2_hat = self.mlp(Zb)\n",
        "        resid = X2b - X2_hat\n",
        "        Sigma, L = self._Sigma()\n",
        "        Y = torch.linalg.solve_triangular(L, resid.T, upper=False)\n",
        "        quad = (Y**2).sum()\n",
        "        logdet = 2.0 * torch.log(torch.clamp(torch.diag(L), min=1e-6)).sum()\n",
        "        m = resid.shape[0]\n",
        "        reg = 1e-3 * torch.norm(self.Lpar)**2\n",
        "        return 0.5 * (quad + m * logdet) + reg\n",
        "\n",
        "def train_one_fold(Ztr, X2tr, Zva, X2va, cfg: NNConfig):\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    device = torch.device(cfg.device)\n",
        "    Ztr_t = torch.tensor(Ztr, dtype=torch.float32, device=device)\n",
        "    X2tr_t = torch.tensor(X2tr, dtype=torch.float32, device=device)\n",
        "    Zva_t = torch.tensor(Zva, dtype=torch.float32, device=device)\n",
        "    X2va_t = torch.tensor(X2va, dtype=torch.float32, device=device)\n",
        "\n",
        "    model = RF_NN(Ztr.shape[1], X2tr.shape[1], cfg.hidden, cfg.dropout).to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < 100:\n",
        "            return 0.1 + 0.9 * (epoch / 100)\n",
        "        else:\n",
        "            return 0.5 * (1 + math.cos(math.pi * (epoch - 100) / (cfg.epochs - 100)))\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "    ntr = Ztr.shape[0]\n",
        "    steps = max(1, ntr // cfg.batch_size)\n",
        "    best = np.inf\n",
        "    best_state = None\n",
        "    patience, wait = 30, 0\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        perm = torch.randperm(ntr, device=device)\n",
        "        for s in range(steps):\n",
        "            si = s * cfg.batch_size\n",
        "            ei = min((s+1) * cfg.batch_size, ntr)\n",
        "            if si >= ei: break\n",
        "            idx = perm[si:ei]\n",
        "            loss = model.nll(Ztr_t[idx], X2tr_t[idx])\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            opt.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_loss = model.nll(Zva_t, X2va_t).item()\n",
        "            if val_loss < best - 1e-6:\n",
        "                best = val_loss\n",
        "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "                wait = 0\n",
        "            else:\n",
        "                wait += 1\n",
        "            if wait >= patience:\n",
        "                break\n",
        "        scheduler.step()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def shrink_cov(S: np.ndarray, rho: float = 0.15) -> np.ndarray:\n",
        "    \"\"\"Ledoit–Wolf style shrinkage to stabilize Σ^{-1} at small n.\"\"\"\n",
        "    S = 0.5*(S + S.T)\n",
        "    k = S.shape[0]\n",
        "    tau = np.trace(S) / max(k, 1)\n",
        "    S_shrunk = (1.0 - rho) * S + rho * (tau * np.eye(k))\n",
        "    return make_spd_from_symmetric(S_shrunk, jitter=1e-8)\n",
        "\n",
        "def crossfit_nn_l2(Z: np.ndarray, X2: np.ndarray, cfg: NNConfig, rho_shrink: float = 0.15):\n",
        "    \"\"\"K-fold cross-fitted NN reduced form + shrunk Σ̂_{ηη}.\"\"\"\n",
        "    n, _ = Z.shape\n",
        "    k2 = X2.shape[1]\n",
        "    eta_hat = np.zeros((n, k2), dtype=np.float64)\n",
        "\n",
        "    for (tr, te) in kfold_indices(n, cfg.K, seed=cfg.seed):\n",
        "        Ztr, Zte = Z[tr], Z[te]\n",
        "        X2tr, X2te = X2[tr], X2[te]\n",
        "\n",
        "        Ztr_std, Zmu, Zsd = standardize_fit(Ztr)\n",
        "        Zte_std = standardize_apply(Zte, Zmu, Zsd)\n",
        "\n",
        "        X2tr_std, X2mu, X2sd = standardize_fit(X2tr)\n",
        "        X2te_std = standardize_apply(X2te, X2mu, X2sd)\n",
        "\n",
        "        ntr = Ztr_std.shape[0]\n",
        "        nva = max(6, int(0.10 * ntr))\n",
        "        va_idx = np.arange(ntr)[:nva]\n",
        "        tr_idx = np.arange(ntr)[nva:]\n",
        "\n",
        "        model = train_one_fold(\n",
        "            Ztr_std[tr_idx], X2tr_std[tr_idx],\n",
        "            Ztr_std[va_idx],  X2tr_std[va_idx],\n",
        "            cfg\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            device = torch.device(cfg.device)\n",
        "            Zte_t = torch.tensor(Zte_std, dtype=torch.float32, device=device)\n",
        "            X2_hat_te_std = model.mlp(Zte_t).cpu().numpy()\n",
        "\n",
        "        # back to original scale\n",
        "        X2_hat_te = X2_hat_te_std * X2sd + X2mu\n",
        "        eta_hat[te] = X2te - X2_hat_te\n",
        "\n",
        "    S = np.cov(eta_hat, rowvar=False)\n",
        "    S = make_spd_from_symmetric(S, jitter=1e-8)\n",
        "    S_shrunk = shrink_cov(S, rho=rho_shrink)\n",
        "    return eta_hat, S_shrunk\n",
        "\n",
        "# ==============================\n",
        "# LIML L1 (fixed Σ̂ηη from NN)\n",
        "# ==============================\n",
        "def lnL1_vec_fixed_sigee(beta, sig_ev, sigee, log_sigma_v, log_lambda_c, y, X, etas):\n",
        "    \"\"\"Per-observation log-likelihood for the L1 (ε=v-u) model.\"\"\"\n",
        "    sigee = make_spd_from_symmetric(sigee, jitter=1e-10)\n",
        "    cF = cho_factor(sigee, lower=False, check_finite=False)\n",
        "    w = cho_solve(cF, sig_ev, check_finite=False)          # w = Σ_{ηη}^{-1} Σ_{ηv}\n",
        "    muci = etas @ w                                        # control function\n",
        "    r = y - X @ beta - muci\n",
        "\n",
        "    sigma_v = np.exp(log_sigma_v)\n",
        "    lam_c   = np.exp(log_lambda_c)                         # λ_c = σ_u/σ_c\n",
        "    correction = float(sig_ev @ w)                         # Σ_{vη} Σ_{ηη}^{-1} Σ_{ηv}\n",
        "    sigma_c2 = sigma_v**2 - correction\n",
        "    if sigma_c2 <= 1e-12:\n",
        "        return np.full(y.shape[0], -1e20)                  # impossible region\n",
        "    sigma_c = np.sqrt(sigma_c2)\n",
        "    sigma2  = sigma_c2 + (lam_c * sigma_c)**2\n",
        "    sigma   = np.sqrt(sigma2)\n",
        "\n",
        "    z = -(lam_c / sigma) * r\n",
        "    z = np.clip(z, -10, 10)                                # numeric guard\n",
        "    lPhi = norm.logcdf(z)\n",
        "    return -0.5*np.log(sigma2) - 0.5*(r**2 / sigma2) + lPhi\n",
        "\n",
        "def neg_loglik_L1_only_with_penalty(theta, y, X, etas, sigee, k2):\n",
        "    \"\"\"Total negative log-likelihood + soft regularization.\"\"\"\n",
        "    p = X.shape[1]\n",
        "    beta = theta[:p]\n",
        "    sig_ev = theta[p:p+k2]\n",
        "    log_sigma_v = theta[-2]\n",
        "    log_lambda_c = theta[-1]\n",
        "\n",
        "    l1 = lnL1_vec_fixed_sigee(beta, sig_ev, sigee, log_sigma_v, log_lambda_c, y, X, etas)\n",
        "    val = -float(np.sum(l1))\n",
        "    if not np.isfinite(val):\n",
        "        return 1e20\n",
        "\n",
        "    # feasibility guard\n",
        "    cF = cho_factor(sigee, lower=False, check_finite=False)\n",
        "    w = cho_solve(cF, sig_ev, check_finite=False)\n",
        "    correction = float(sig_ev @ w)\n",
        "    sigma_v = np.exp(log_sigma_v)\n",
        "    sigma_c2 = sigma_v**2 - correction\n",
        "    if sigma_c2 <= 1e-12:\n",
        "        return 1e20\n",
        "\n",
        "    # softer priors\n",
        "    sigma_v_center  = 0.05 * (log_sigma_v - 0.0)**2     # gentle center at log(1)\n",
        "    lambda_c_center = 0.10 * (log_lambda_c - np.log(0.3))**2\n",
        "    impossible_penalty = 1e3 * max(0, -sigma_c2)\n",
        "    param_penalty = 0.02 * (np.sum(beta**2) + np.sum(sig_ev**2))\n",
        "\n",
        "    return val + sigma_v_center + lambda_c_center + impossible_penalty + param_penalty\n",
        "\n",
        "def estimate_LIML_L1_only(y, X, etas, sigee, fallback=True):\n",
        "    \"\"\"Optimize the L1 likelihood for (β, Σ_{ηv}, σ_v, λ_c) given Σ̂_{ηη}.\"\"\"\n",
        "    n, p = X.shape\n",
        "    k2   = etas.shape[1]\n",
        "\n",
        "    beta0 = np.linalg.lstsq(X, y, rcond=1e-8)[0]\n",
        "    e0 = y - X @ beta0\n",
        "    try:\n",
        "        sig_ev0 = np.linalg.lstsq(etas, e0, rcond=1e-8)[0]\n",
        "    except np.linalg.LinAlgError:\n",
        "        sig_ev0 = np.zeros(k2)\n",
        "\n",
        "    # multiple starts for variance params\n",
        "    starts_sigma_v = [np.log(1.0), np.log(0.6), np.log(1.5), np.log(2.0)]\n",
        "    starts_lambda_c = [np.log(0.3), np.log(0.5), np.log(0.1), np.log(0.8)]\n",
        "\n",
        "    # WIDER bounds, allow σ_v down to 0.10\n",
        "    lower = [-np.inf]*p + [-5.0]*k2 + [np.log(0.10), np.log(0.02)]\n",
        "    upper = [ np.inf]*p + [ 5.0]*k2 + [np.log(5.00), np.log(3.00)]\n",
        "    bounds = list(zip(lower, upper))\n",
        "\n",
        "    theta0_list = []\n",
        "    for sv in starts_sigma_v:\n",
        "        for lc in starts_lambda_c:\n",
        "            theta0_list.append(np.concatenate([beta0, sig_ev0, [sv, lc]]))\n",
        "\n",
        "    methods = ['L-BFGS-B'] + (['SLSQP'] if fallback else [])\n",
        "    best_res, best_fun = None, np.inf\n",
        "    for theta0 in theta0_list:\n",
        "        for m in methods:\n",
        "            try:\n",
        "                res = minimize(neg_loglik_L1_only_with_penalty, theta0,\n",
        "                               args=(y, X, etas, sigee, k2),\n",
        "                               method=m, bounds=bounds,\n",
        "                               options={'maxiter': 8000, 'ftol': 1e-9})\n",
        "                if res.fun < best_fun:\n",
        "                    best_res, best_fun = res, res.fun\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"LIML optimization failed with {m}: {e}\")\n",
        "\n",
        "    if best_res is None:\n",
        "        raise RuntimeError(\"All optimization attempts failed.\")\n",
        "\n",
        "    theta = best_res.x\n",
        "    beta_hat = theta[:p]\n",
        "    sig_ev_hat = theta[p:p+k2]\n",
        "    log_sigma_v = theta[-2]\n",
        "    log_lambda_c = theta[-1]\n",
        "\n",
        "    cF = cho_factor(sigee, lower=False, check_finite=False)\n",
        "    w = cho_solve(cF, sig_ev_hat, check_finite=False)\n",
        "    correction = float(sig_ev_hat @ w)\n",
        "\n",
        "    sigma_v_hat = float(np.exp(log_sigma_v))\n",
        "    sigma_c2_hat = sigma_v_hat**2 - correction\n",
        "    sigma_c_hat = float(np.sqrt(max(sigma_c2_hat, 1e-15)))\n",
        "    lambda_c_hat = float(np.exp(log_lambda_c))\n",
        "    sigma_u_hat = float(lambda_c_hat * sigma_c_hat)\n",
        "\n",
        "    out = {\n",
        "        'success': best_res.success,\n",
        "        'message': best_res.message,\n",
        "        'nit': getattr(best_res, 'nit', None),\n",
        "        'nfev': getattr(best_res, 'nfev', None),\n",
        "        'beta': beta_hat, 'sig_ev': sig_ev_hat,\n",
        "        'sigma_v': sigma_v_hat, 'sigma_c': sigma_c_hat,\n",
        "        'sigma_u': sigma_u_hat, 'lambda_c': lambda_c_hat,\n",
        "        'w': w, 'theta': theta,\n",
        "        'bounds': {'lower': lower, 'upper': upper}\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# ==============================\n",
        "# Inference helpers (OPG)\n",
        "# ==============================\n",
        "def _pack_theta(beta, sig_ev, log_sigma_v, log_lambda_c):\n",
        "    return np.concatenate([beta, sig_ev, [log_sigma_v, log_lambda_c]])\n",
        "\n",
        "def _unpack_theta(theta, p, k2):\n",
        "    beta = theta[:p]\n",
        "    sig_ev = theta[p:p+k2]\n",
        "    log_sigma_v = theta[-2]\n",
        "    log_lambda_c = theta[-1]\n",
        "    return beta, sig_ev, log_sigma_v, log_lambda_c\n",
        "\n",
        "def ll_obs(theta, y, X, etas, sigee):\n",
        "    p, k2 = X.shape[1], etas.shape[1]\n",
        "    beta, sig_ev, lsv, llc = _unpack_theta(theta, p, k2)\n",
        "    return lnL1_vec_fixed_sigee(beta, sig_ev, sigee, lsv, llc, y, X, etas)\n",
        "\n",
        "def opg_vcov(theta_hat, y, X, etas, sigee, eps=1e-6):\n",
        "    n = y.shape[0]\n",
        "    k = theta_hat.size\n",
        "    base = ll_obs(theta_hat, y, X, etas, sigee)  # (n,)\n",
        "    G = np.empty((n, k))\n",
        "    for j in range(k):\n",
        "        th = theta_hat.copy()\n",
        "        th[j] += eps\n",
        "        fwd = ll_obs(th, y, X, etas, sigee)\n",
        "        G[:, j] = (fwd - base) / eps\n",
        "    OPG = G.T @ G\n",
        "    try:\n",
        "        vcov = np.linalg.inv(OPG)\n",
        "    except np.linalg.LinAlgError:\n",
        "        vcov = np.linalg.pinv(OPG)\n",
        "    return vcov\n",
        "\n",
        "def gradient_at(theta_hat, y, X, etas, sigee, eps=1e-6):\n",
        "    k = theta_hat.size\n",
        "    base = ll_obs(theta_hat, y, X, etas, sigee)\n",
        "    g = np.zeros(k)\n",
        "    for j in range(k):\n",
        "        th = theta_hat.copy()\n",
        "        th[j] += eps\n",
        "        fwd = ll_obs(th, y, X, etas, sigee)\n",
        "        g[j] = np.sum((fwd - base) / eps)  # gradient of total loglik\n",
        "    return g\n",
        "\n",
        "def lambda_c_profile(theta_hat, y, X, etas, sigee, grid=None):\n",
        "    \"\"\"Diagnostic profile: vary λ_c holding other params fixed (for quick sanity).\"\"\"\n",
        "    p, k2 = X.shape[1], etas.shape[1]\n",
        "    if grid is None:\n",
        "        lam0 = float(np.exp(theta_hat[-1]))\n",
        "        grid = np.array([lam0*0.25, lam0*0.5, lam0, lam0*2, lam0*4])\n",
        "        grid = np.clip(grid, 0.01, 5.0)\n",
        "    vals = []\n",
        "    for lam in grid:\n",
        "        th = theta_hat.copy()\n",
        "        th[-1] = np.log(lam)\n",
        "        vals.append(np.sum(ll_obs(th, y, X, etas, sigee)))\n",
        "    return grid, np.array(vals)\n",
        "\n",
        "# ==============================\n",
        "# Real data preparation (paper mapping)\n",
        "# ==============================\n",
        "DATA_PATHS = [\n",
        "    \"/content/SPANISH DATASET.csv\",     # CSV (uploaded)\n",
        "    \"/content/SPANISH DATASET.xlsx\",     # Excel (fallback)\n",
        "]\n",
        "\n",
        "USECOLS = ['DEPREC', 'FARMEXP', 'MILK',\n",
        "           'LABOR', 'COWS', 'FEED', 'LAND',\n",
        "           'YEAR', 'COOP', 'PFEED', 'PMILK', 'LANDOWN',\n",
        "           'COUNTY', 'ZONE']  # COUNTY/ZONE optional; code will handle if absent\n",
        "\n",
        "def load_dataset() -> pd.DataFrame:\n",
        "    path = None\n",
        "    for p in DATA_PATHS:\n",
        "        if os.path.exists(p):\n",
        "            path = p\n",
        "            break\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(\"Could not find dataset. Checked: \" + \", \".join(DATA_PATHS))\n",
        "\n",
        "    if path.lower().endswith(\".csv\"):\n",
        "        df = pd.read_csv(path)\n",
        "    else:\n",
        "        df = pd.read_excel(path)\n",
        "\n",
        "    missing = [c for c in USECOLS if c not in df.columns]\n",
        "    # Allow COUNTY/ZONE to be missing\n",
        "    soft_missing = [c for c in missing if c in ('COUNTY', 'ZONE')]\n",
        "    hard_missing = [c for c in missing if c not in ('COUNTY', 'ZONE')]\n",
        "    if hard_missing:\n",
        "        raise ValueError(f\"Missing columns in data: {hard_missing}\")\n",
        "\n",
        "    keep_cols = [c for c in USECOLS if (c in df.columns)]\n",
        "    df = df[keep_cols].copy()\n",
        "\n",
        "    # Drop zero-roughage farms\n",
        "    df = df[(df['DEPREC'] != 0) & (df['FARMEXP'] != 0)].copy()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df['ROUGHAGE'] = df['FARMEXP'] + df['DEPREC']\n",
        "    return df\n",
        "\n",
        "def build_design_matrices(df: pd.DataFrame):\n",
        "    # y = log(MILK) demeaned\n",
        "    y = np.log(df['MILK'].values) - np.log(df['MILK'].values).mean()\n",
        "\n",
        "    # Structural X: CONST + log(LABOR,COWS,FEED,LAND,ROUGHAGE) demeaned + YEAR dummies\n",
        "    X_raw = df[['LABOR','COWS','FEED','LAND','ROUGHAGE']].copy()\n",
        "    X_log = np.log(X_raw.values)\n",
        "    X_log = X_log - X_log.mean(axis=0, keepdims=True)\n",
        "    X_df = pd.DataFrame(X_log, columns=['LABOR','COWS','FEED','LAND','ROUGHAGE'])\n",
        "    X_df.insert(0, 'CONST', 1.0)\n",
        "\n",
        "    YEAR_dummy = pd.get_dummies(df['YEAR'], prefix='YEAR', drop_first=True, dtype=int)\n",
        "    X_df = pd.concat([X_df, YEAR_dummy], axis=1)\n",
        "\n",
        "    # Base instruments Z (A): CONST, LAND, prices, ownership, COOP, YEAR\n",
        "    Z_parts = []\n",
        "    Z_base = pd.DataFrame({'CONST': np.ones(len(df))})\n",
        "    Z_parts.append(Z_base)\n",
        "\n",
        "    Z_parts.append(X_df[['LAND']])  # LAND from X (already log-de-meaned)\n",
        "    for col in ['PMILK','PFEED','LANDOWN']:\n",
        "        if col in df.columns:\n",
        "            Z_parts.append(df[[col]])\n",
        "\n",
        "    # COOP dummies\n",
        "    if 'COOP' in df.columns:\n",
        "        COOP_dummy = pd.get_dummies(df['COOP'], prefix='COOP', drop_first=True, dtype=int)\n",
        "        Z_parts.append(COOP_dummy)\n",
        "\n",
        "    # YEAR dummies\n",
        "    Z_parts.append(YEAR_dummy)\n",
        "\n",
        "    # Optional geographic dummies if present\n",
        "    if 'COUNTY' in df.columns:\n",
        "        COUNTY_dummy = pd.get_dummies(df['COUNTY'], prefix='COUNTY', drop_first=True, dtype=int)\n",
        "        Z_parts.append(COUNTY_dummy)\n",
        "    if 'ZONE' in df.columns:\n",
        "        ZONE_dummy = pd.get_dummies(df['ZONE'], prefix='ZONE', drop_first=True, dtype=int)\n",
        "        Z_parts.append(ZONE_dummy)\n",
        "\n",
        "    Z_df_A = pd.concat(Z_parts, axis=1)\n",
        "\n",
        "    # (B) Squares of continuous instruments\n",
        "    cont_cols = [c for c in ['LAND','PMILK','PFEED','LANDOWN'] if c in Z_df_A.columns]\n",
        "    Z_sq = pd.DataFrame(index=Z_df_A.index)\n",
        "    for c in cont_cols:\n",
        "        Z_sq[c+'_SQ'] = Z_df_A[c]**2\n",
        "\n",
        "    # (C) Selected interactions: LAND×prices, price×price, and LAND×LANDOWN\n",
        "    Z_int = pd.DataFrame(index=Z_df_A.index)\n",
        "    if 'LAND' in Z_df_A.columns and 'PMILK' in Z_df_A.columns:\n",
        "        Z_int['LAND_PMILK'] = Z_df_A['LAND'] * Z_df_A['PMILK']\n",
        "    if 'LAND' in Z_df_A.columns and 'PFEED' in Z_df_A.columns:\n",
        "        Z_int['LAND_PFEED'] = Z_df_A['LAND'] * Z_df_A['PFEED']\n",
        "    if 'PMILK' in Z_df_A.columns and 'PFEED' in Z_df_A.columns:\n",
        "        Z_int['PMILK_PFEED'] = Z_df_A['PMILK'] * Z_df_A['PFEED']\n",
        "    if 'LAND' in Z_df_A.columns and 'LANDOWN' in Z_df_A.columns:\n",
        "        Z_int['LAND_LANDOWN'] = Z_df_A['LAND'] * Z_df_A['LANDOWN']\n",
        "\n",
        "    # Final Z (A + B + C)\n",
        "    Z_df = pd.concat([Z_df_A, Z_sq, Z_int], axis=1)\n",
        "\n",
        "    # Endogenous block X2 (for NN): LABOR, COWS, FEED, ROUGHAGE\n",
        "    X2_cols = ['LABOR','COWS','FEED','ROUGHAGE']\n",
        "    X2 = X_df[X2_cols].values\n",
        "\n",
        "    # Structural X: CONST, LABOR, COWS, FEED, LAND, ROUGHAGE + YEAR dummies\n",
        "    struct_cols = ['CONST','LABOR','COWS','FEED','LAND','ROUGHAGE'] + list(YEAR_dummy.columns)\n",
        "    X_struct = X_df[struct_cols].values\n",
        "    X_struct_cols = struct_cols\n",
        "\n",
        "    return y, X_struct, X_struct_cols, X2, Z_df.values, Z_df.columns\n",
        "\n",
        "# ==============================\n",
        "# Baseline (ElasticNet) helper\n",
        "# ==============================\n",
        "def crossfit_elasticnet_r2(Z: np.ndarray, X2: np.ndarray, K: int = 10, seed: int = BASE_SEED):\n",
        "    \"\"\"K-fold cross-fitted ElasticNetCV baseline per endogenous regressor.\"\"\"\n",
        "    n, _ = Z.shape\n",
        "    k2 = X2.shape[1]\n",
        "    preds = np.zeros_like(X2)\n",
        "    nnz = np.zeros(k2, dtype=int)\n",
        "    kf = KFold(n_splits=K, shuffle=True, random_state=seed)\n",
        "    for j in range(k2):\n",
        "        fold_nnz = []\n",
        "        for tr, te in kf.split(np.arange(n)):\n",
        "            Ztr, Zte = Z[tr], Z[te]\n",
        "            xtr, xte = X2[tr, j], X2[te, j]\n",
        "            Ztr_std, mu, sd = standardize_fit(Ztr)\n",
        "            Zte_std = standardize_apply(Zte, mu, sd)\n",
        "            encv = ElasticNetCV(\n",
        "                l1_ratio=[0.2, 0.5, 0.8, 1.0],  # includes pure Lasso\n",
        "                cv=5, random_state=seed, max_iter=10000\n",
        "            )\n",
        "            encv.fit(Ztr_std, xtr)\n",
        "            preds[te, j] = encv.predict(Zte_std)\n",
        "            fold_nnz.append(int(np.count_nonzero(encv.coef_)))\n",
        "        nnz[j] = int(np.round(np.mean(fold_nnz)))\n",
        "    r2s = [r2_score(X2[:, j], preds[:, j]) for j in range(k2)]\n",
        "    return r2s, nnz.tolist()\n",
        "\n",
        "# ==============================\n",
        "# Main real-data run\n",
        "# ==============================\n",
        "def run_realdata(cfg: Optional[NNConfig] = None, rho_shrink: float = 0.15):\n",
        "    if cfg is None:\n",
        "        cfg = NNConfig()\n",
        "\n",
        "    df = load_dataset()\n",
        "    y, X_struct, Xcols, X2, Z_raw, Zcolnames = build_design_matrices(df)\n",
        "    n = y.shape[0]\n",
        "\n",
        "    # --- Report Z diagnostics (count, standardized condition number) ---\n",
        "    print(\"\\n== First-stage diagnostics ==\")\n",
        "    print(f\"Z columns used (for OLS/Lasso): {Z_raw.shape[1]}\")\n",
        "\n",
        "    Zstd_full, muZ_full, sdZ_full = standardize_fit(Z_raw.astype(float))\n",
        "    Zstd_noconst = Zstd_full[:, 1:] if Zstd_full.shape[1] > 1 else Zstd_full\n",
        "    try:\n",
        "        svals = np.linalg.svd(Zstd_noconst, full_matrices=False)[1]\n",
        "        cond_Z = float(svals.max() / max(svals.min(), 1e-12))\n",
        "    except Exception:\n",
        "        cond_Z = np.nan\n",
        "    print(f\"Cond. number of Z (standardized, no CONST): {cond_Z:,.2f}\")\n",
        "\n",
        "    # --- PCA for NN (and optionally for OLS/EN) ---\n",
        "    Z_for_nn = Z_raw.copy()\n",
        "    m_kept = None\n",
        "    if PCA_Z_FOR_NN:\n",
        "        pca = PCA(n_components=None, svd_solver='full', random_state=BASE_SEED)\n",
        "        Zp = pca.fit_transform(Zstd_full)\n",
        "        cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
        "        m_kept = int(np.searchsorted(cumvar, PCA_VAR_KEEP) + 1)\n",
        "        Z_for_nn = Zp[:, :m_kept]\n",
        "        print(f\"Z PCA -> components kept for NN: {m_kept} (variance ≥ {PCA_VAR_KEEP*100:.2f}%)\")\n",
        "    else:\n",
        "        print(\"Z PCA disabled for NN.\")\n",
        "\n",
        "    Z_for_ols = Z_raw.copy()\n",
        "    if PCA_Z_FOR_NN and PCA_Z_FOR_OLS_LASSO:\n",
        "        # reuse the same PCA fit (on standardized Z)\n",
        "        Z_for_ols = Zp[:, :m_kept]\n",
        "        print(\"Using same PCA-compressed Z for OLS/ElasticNet baselines.\")\n",
        "    else:\n",
        "        print(\"Baselines use raw Z.\")\n",
        "\n",
        "    # --- First-stage OLS baseline (no crossfit) ---\n",
        "    ols_r2 = []\n",
        "    for j in range(X2.shape[1]):\n",
        "        lm = LinearRegression().fit(Z_for_ols, X2[:, j])\n",
        "        ols_r2.append(float(lm.score(Z_for_ols, X2[:, j])))\n",
        "\n",
        "    # --- ElasticNet (cross-fitted) baseline ---\n",
        "    en_r2, en_nnz = crossfit_elasticnet_r2(Z_for_ols, X2, K=cfg.K, seed=cfg.seed)\n",
        "\n",
        "    # --- NN-L2 with shrinkage Σ̂ηη ---\n",
        "    eta_hat, Sigma_hat = crossfit_nn_l2(Z_for_nn, X2, cfg, rho_shrink=rho_shrink)\n",
        "    X2_hat = X2 - eta_hat\n",
        "    nn_r2 = [r2_score(X2[:, j], X2_hat[:, j]) for j in range(X2.shape[1])]\n",
        "\n",
        "    print(f\"OLS_R2_per_endog   (LABOR,COWS,FEED,ROUGHAGE): {_round_list(ols_r2)}\")\n",
        "    print(f\"ElasticNet_R2_per_endog (LABOR,COWS,FEED,ROUGHAGE): {_round_list(en_r2)}\")\n",
        "    print(f\"ElasticNet mean #nonzero per endog:                  {en_nnz}\")\n",
        "    print(f\"NN_R2_per_endog    (LABOR,COWS,FEED,ROUGHAGE): {_round_list(nn_r2)}\")\n",
        "\n",
        "\n",
        "    # --- LIML L1 with fixed Σ̂ηη ---\n",
        "    liml = estimate_LIML_L1_only(y, X_struct, eta_hat, Sigma_hat, fallback=True)\n",
        "\n",
        "    print(\"\\n== Optimizer ==\")\n",
        "    print(\"success :\", liml['success'])\n",
        "    print(\"message :\", liml['message'])\n",
        "    if liml.get('nit') is not None:\n",
        "        print(\"nit     :\", liml['nit'])\n",
        "    if liml.get('nfev') is not None:\n",
        "        print(\"nfev    :\", liml['nfev'])\n",
        "\n",
        "    # Bound hit check (for variance params)\n",
        "    lb, ub = liml['bounds']['lower'], liml['bounds']['upper']\n",
        "    log_sigma_v_hat = np.log(liml['sigma_v'])\n",
        "    log_lambda_c_hat = np.log(liml['lambda_c'])\n",
        "    print(\"\\n== Bound check ==\")\n",
        "    print(\"hit log_sigma_v lower? \", np.isclose(log_sigma_v_hat, lb[-2]))\n",
        "    print(\"hit log_sigma_v upper? \", np.isclose(log_sigma_v_hat, ub[-2]))\n",
        "    print(\"hit log_lambda_c lower?\", np.isclose(log_lambda_c_hat, lb[-1]))\n",
        "    print(\"hit log_lambda_c upper?\", np.isclose(log_lambda_c_hat, ub[-1]))\n",
        "\n",
        "    # --- Structural betas ---\n",
        "    betadf = pd.DataFrame(liml['beta'], index=Xcols, columns=['coef'])\n",
        "    print(\"\\n== Structural betas ==\")\n",
        "    print(betadf.round(4))\n",
        "\n",
        "    # --- Variance components ---\n",
        "    vardf = pd.DataFrame({\n",
        "        'sigma_v': [liml['sigma_v']],\n",
        "        'sigma_c': [liml['sigma_c']],\n",
        "        'sigma_u': [liml['sigma_u']],\n",
        "        'lambda_c': [liml['lambda_c']]\n",
        "    })\n",
        "    print(\"\\n== Variance components ==\")\n",
        "    print(vardf.round(4))\n",
        "\n",
        "    # --- Inference (OPG) ---\n",
        "    p, k2 = X_struct.shape[1], eta_hat.shape[1]\n",
        "    theta_hat = _pack_theta(liml['beta'], liml['sig_ev'],\n",
        "                            np.log(liml['sigma_v']), np.log(liml['lambda_c']))\n",
        "    vcov = opg_vcov(theta_hat, y, X_struct, eta_hat, Sigma_hat, eps=1e-6)\n",
        "    se = np.sqrt(np.clip(np.diag(vcov), 0, np.inf))\n",
        "    param_names = Xcols + [f\"sig_ev[{j}]\" for j in range(k2)] + [\"log_sigma_v\",\"log_lambda_c\"]\n",
        "    est = np.r_[liml['beta'], liml['sig_ev'], np.log(liml['sigma_v']), np.log(liml['lambda_c'])]\n",
        "    ci_lo = est - 1.96*se\n",
        "    ci_hi = est + 1.96*se\n",
        "    inference_tbl = pd.DataFrame({\"est\": est, \"se\": se, \"ci_lo\": ci_lo, \"ci_hi\": ci_hi}, index=param_names)\n",
        "\n",
        "    print(\"\\n== Inference (OPG-based) ==\")\n",
        "    print(inference_tbl.round(4))\n",
        "\n",
        "    # --- Technical Efficiency (TE) ---\n",
        "    muci = eta_hat @ liml['w']\n",
        "    r = y - X_struct @ liml['beta'] - muci\n",
        "    te = te_from_r_sigma(r, liml['sigma_u'], liml['sigma_c'])\n",
        "    te_summary = {\n",
        "        \"TE_mean\": float(np.mean(te)),\n",
        "        \"TE_median\": float(np.median(te)),\n",
        "        \"TE_p10\": float(np.percentile(te, 10)),\n",
        "        \"TE_p25\": float(np.percentile(te, 25)),\n",
        "        \"TE_p75\": float(np.percentile(te, 75)),\n",
        "        \"TE_p90\": float(np.percentile(te, 90))\n",
        "    }\n",
        "    print(\"\\n== Technical Efficiency (TE) ==\")\n",
        "    print(pd.Series(te_summary).round(4))\n",
        "\n",
        "    # --- Diagnostics ---\n",
        "    ll = float(np.sum(ll_obs(theta_hat, y, X_struct, eta_hat, Sigma_hat)))\n",
        "    yhat = X_struct @ liml['beta'] + muci\n",
        "    sst = np.sum((y - y.mean())**2)\n",
        "    sse = np.sum((y - yhat)**2)\n",
        "    pseudo_R2 = 1.0 - sse / max(sst, 1e-12)\n",
        "    grad = gradient_at(theta_hat, y, X_struct, eta_hat, Sigma_hat, eps=1e-6)\n",
        "    grad_norm = float(np.linalg.norm(grad, ord=2))\n",
        "\n",
        "    print(\"\\n== Diagnostics ==\")\n",
        "    print(f\"logLik (L1): {ll:.4f}\")\n",
        "    print(f\"pseudo-R^2 (structural): {pseudo_R2:.4f}\")\n",
        "    print(f\"||grad||_2 at optimum: {grad_norm:.3e}\")\n",
        "\n",
        "    # --- Σ̂ηη spectrum / identification check ---\n",
        "    evals = np.linalg.eigvalsh(Sigma_hat)\n",
        "    cond  = evals.max() / max(evals.min(), 1e-12)\n",
        "    print(\"\\n== Σ̂_{ηη} spectrum ==\")\n",
        "    print(\"eigenvalues:\", np.round(evals, 6).tolist())\n",
        "    print(\"condition number:\", round(cond, 2))\n",
        "\n",
        "    # --- λ_c profile (diagnostic) ---\n",
        "    grid, vals = lambda_c_profile(theta_hat, y, X_struct, eta_hat, Sigma_hat)\n",
        "    prof = pd.DataFrame({\"lambda_c\": grid, \"logLik_L1\": vals})\n",
        "    print(\"\\n== λ_c profile (diagnostic; other params held fixed) ==\")\n",
        "    print(prof.round(6).to_string(index=False))\n",
        "\n",
        "    # Return everything in case you want to save/report\n",
        "    out: Dict[str, Any] = {\n",
        "        \"betas\": betadf,\n",
        "        \"variances\": vardf,\n",
        "        \"inference\": inference_tbl,\n",
        "        \"TE\": te,\n",
        "        \"TE_summary\": te_summary,\n",
        "        \"logLik_L1\": ll,\n",
        "        \"pseudo_R2\": pseudo_R2,\n",
        "        \"grad_norm\": grad_norm,\n",
        "        \"Sigma_eta_eta\": Sigma_hat,\n",
        "        \"Sigma_eigvals\": evals,\n",
        "        \"Sigma_cond\": cond,\n",
        "        \"first_stage_R2_OLS\": ols_r2,\n",
        "        \"first_stage_R2_ElasticNet\": en_r2,\n",
        "        \"ElasticNet_n_nonzero\": en_nnz,\n",
        "        \"first_stage_R2_NN\": nn_r2,\n",
        "        \"lambda_c_profile\": prof,\n",
        "        \"liml\": liml,\n",
        "        \"Xcols\": Xcols,\n",
        "        \"Z_cond_std_no_const\": cond_Z,\n",
        "        \"PCA_components_kept\": m_kept if PCA_Z_FOR_NN else None,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# ---------------- Main --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n=== Real-data NN-LIML (endog: LABOR, COWS, FEED, ROUGHAGE) ===\")\n",
        "    cfg = NNConfig()\n",
        "    _ = run_realdata(cfg=cfg, rho_shrink=0.15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPcYJEd0nHaE",
        "outputId": "d469b7d9-28e3-4fe0-a80b-ab8ed3907900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Real-data NN-LIML (endog: LABOR, COWS, FEED, ROUGHAGE) ===\n",
            "\n",
            "== First-stage diagnostics ==\n",
            "Z columns used (for OLS/Lasso): 53\n",
            "Cond. number of Z (standardized, no CONST): 90,747,661,659,658.56\n",
            "Z PCA disabled for NN.\n",
            "Baselines use raw Z.\n",
            "OLS_R2_per_endog   (LABOR,COWS,FEED,ROUGHAGE): [0.5406, 0.7075, 0.6498, 0.6236]\n",
            "ElasticNet_R2_per_endog (LABOR,COWS,FEED,ROUGHAGE): [0.4999, 0.6698, 0.605, 0.5872]\n",
            "ElasticNet mean #nonzero per endog:                  [36, 39, 38, 34]\n",
            "NN_R2_per_endog    (LABOR,COWS,FEED,ROUGHAGE): [0.636, 0.7362, 0.6658, 0.6426]\n",
            "\n",
            "== Optimizer ==\n",
            "success : True\n",
            "message : Optimization terminated successfully\n",
            "nit     : 212\n",
            "nfev    : 5262\n",
            "\n",
            "== Bound check ==\n",
            "hit log_sigma_v lower?  False\n",
            "hit log_sigma_v upper?  False\n",
            "hit log_lambda_c lower? False\n",
            "hit log_lambda_c upper? False\n",
            "\n",
            "== Structural betas ==\n",
            "             coef\n",
            "CONST      0.0891\n",
            "LABOR     -0.0395\n",
            "COWS       0.9176\n",
            "FEED       0.1715\n",
            "LAND      -0.0910\n",
            "ROUGHAGE   0.1377\n",
            "YEAR_2000  0.0113\n",
            "YEAR_2001  0.0079\n",
            "YEAR_2002  0.0229\n",
            "YEAR_2003  0.0045\n",
            "YEAR_2004  0.0199\n",
            "YEAR_2005  0.0476\n",
            "YEAR_2006  0.0714\n",
            "YEAR_2007  0.0704\n",
            "YEAR_2008  0.0568\n",
            "YEAR_2009  0.0390\n",
            "YEAR_2010  0.0673\n",
            "\n",
            "== Variance components ==\n",
            "   sigma_v  sigma_c  sigma_u  lambda_c\n",
            "0   0.1161   0.0762   0.1581    2.0750\n",
            "\n",
            "== Inference (OPG-based) ==\n",
            "                 est     se   ci_lo   ci_hi\n",
            "CONST         0.0891 0.0158  0.0581  0.1200\n",
            "LABOR        -0.0395 0.0285 -0.0954  0.0164\n",
            "COWS          0.9176 0.0451  0.8293  1.0059\n",
            "FEED          0.1715 0.0222  0.1280  0.2149\n",
            "LAND         -0.0910 0.0110 -0.1125 -0.0695\n",
            "ROUGHAGE      0.1377 0.0154  0.1075  0.1679\n",
            "YEAR_2000     0.0113 0.0164 -0.0208  0.0435\n",
            "YEAR_2001     0.0079 0.0160 -0.0234  0.0392\n",
            "YEAR_2002     0.0229 0.0168 -0.0102  0.0559\n",
            "YEAR_2003     0.0045 0.0181 -0.0311  0.0401\n",
            "YEAR_2004     0.0199 0.0167 -0.0127  0.0526\n",
            "YEAR_2005     0.0476 0.0214  0.0056  0.0895\n",
            "YEAR_2006     0.0714 0.0216  0.0292  0.1137\n",
            "YEAR_2007     0.0704 0.0213  0.0287  0.1121\n",
            "YEAR_2008     0.0568 0.0212  0.0152  0.0984\n",
            "YEAR_2009     0.0390 0.0221 -0.0042  0.0823\n",
            "YEAR_2010     0.0673 0.0217  0.0247  0.1099\n",
            "sig_ev[0]    -0.0017 0.0021 -0.0058  0.0023\n",
            "sig_ev[1]    -0.0168 0.0019 -0.0205 -0.0132\n",
            "sig_ev[2]     0.0035 0.0018  0.0000  0.0070\n",
            "sig_ev[3]    -0.0171 0.0031 -0.0233 -0.0110\n",
            "log_sigma_v  -2.1532 0.0643 -2.2792 -2.0272\n",
            "log_lambda_c  0.7299 0.0969  0.5400  0.9199\n",
            "\n",
            "== Technical Efficiency (TE) ==\n",
            "TE_mean     0.8858\n",
            "TE_median   0.9030\n",
            "TE_p10      0.7955\n",
            "TE_p25      0.8528\n",
            "TE_p75      0.9342\n",
            "TE_p90      0.9518\n",
            "dtype: float64\n",
            "\n",
            "== Diagnostics ==\n",
            "logLik (L1): 1421.4276\n",
            "pseudo-R^2 (structural): 0.9176\n",
            "||grad||_2 at optimum: 8.431e+00\n",
            "\n",
            "== Σ̂_{ηη} spectrum ==\n",
            "eigenvalues: [0.035255, 0.067003, 0.113172, 0.373858]\n",
            "condition number: 10.6\n",
            "\n",
            "== λ_c profile (diagnostic; other params held fixed) ==\n",
            " lambda_c  logLik_L1\n",
            "   0.5187     2.0122\n",
            "   1.0375   938.5917\n",
            "   2.0750  1421.4276\n",
            "   4.1499  1018.8043\n",
            "   5.0000   813.3179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRZDQhaXnJuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}